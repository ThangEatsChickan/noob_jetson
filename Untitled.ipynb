{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9dc8ad9-fa39-403b-849c-a9d863a62c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf #there is a known bug where TensorFlow needs to be imported before TensorRT\n",
    "import uff # to convert the graph from a serialized frozen TensorFlow model to UFF.\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a58cc7f8-51ee-447f-812c-1c235b94efa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "STARTER_LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 10\n",
    "NUM_CLASSES = 10\n",
    "MAX_STEPS = 3000\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE ** 2\n",
    "OUTPUT_NAMES = [\"fc2/Relu\"]\n",
    "\n",
    "def WeightsVariable(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1, name='weights'))\n",
    "\n",
    "def BiasVariable(shape):\n",
    "    return tf.Variable(tf.constant(0.1, shape=shape, name='biases'))\n",
    "\n",
    "def Conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    filter_size = W.get_shape().as_list()\n",
    "    pad_size = filter_size[0]//2\n",
    "    pad_mat = np.array([[0,0],[pad_size,pad_size],[pad_size,pad_size],[0,0]])\n",
    "    x = tf.pad(x, pad_mat)\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='VALID')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def MaxPool2x2(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    pad_size = k//2\n",
    "    pad_mat = np.array([[0,0],[pad_size,pad_size],[pad_size,pad_size],[0,0]])\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='VALID')\n",
    "\n",
    "def network(images):\n",
    "    # Convolution 1\n",
    "    with tf.name_scope('conv1'):\n",
    "        weights = WeightsVariable([5,5,1,32])\n",
    "        biases = BiasVariable([32])\n",
    "        conv1 = tf.nn.relu(Conv2d(images, weights, biases))\n",
    "        pool1 = MaxPool2x2(conv1)\n",
    "    # Convolution 2\n",
    "    with tf.name_scope('conv2'):\n",
    "        weights = WeightsVariable([5,5,32,64])\n",
    "        biases = BiasVariable([64])\n",
    "        conv2 = tf.nn.relu(Conv2d(pool1, weights, biases))\n",
    "        pool2 = MaxPool2x2(conv2)\n",
    "        pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "    # Fully Connected 1\n",
    "    with tf.name_scope('fc1'):\n",
    "        weights = WeightsVariable([7 * 7 * 64, 1024])\n",
    "        biases = BiasVariable([1024])\n",
    "        fc1 = tf.nn.relu(tf.matmul(pool2_flat, weights) + biases)\n",
    "    # Fully Connected 2\n",
    "    with tf.name_scope('fc2'):\n",
    "        weights = WeightsVariable([1024, 10])\n",
    "        biases = BiasVariable([10])\n",
    "        fc2 = tf.nn.relu(tf.matmul(fc1, weights) + biases)\n",
    "    return fc2\n",
    "\n",
    "def loss_metrics(logits, labels):\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,\n",
    "                                                                   logits=logits,\n",
    "                                                                   name='softmax')\n",
    "    return tf.reduce_mean(cross_entropy, name='softmax_mean')\n",
    "\n",
    "\n",
    "def training(loss):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(STARTER_LEARNING_RATE,\n",
    "                                               global_step,\n",
    "                                               100000,\n",
    "                                               0.75,\n",
    "                                               staircase=True)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    return train_op\n",
    "\n",
    "def evaluation(logits, labels):\n",
    "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "    return tf.reduce_sum(tf.cast(correct, tf.int32))\n",
    "\n",
    "def do_eval(sess,\n",
    "            eval_correct,\n",
    "            images_placeholder,\n",
    "            labels_placeholder,\n",
    "            data_set,\n",
    "            summary):\n",
    "    true_count = 0\n",
    "    steps_per_epoch = data_set.num_examples // BATCH_SIZE\n",
    "    num_examples = steps_per_epoch * BATCH_SIZE\n",
    "    for step in range(steps_per_epoch):\n",
    "        feed_dict = fill_feed_dict(data_set,\n",
    "                                   images_placeholder,\n",
    "                                   labels_placeholder)\n",
    "        log, correctness = sess.run([summary, eval_correct], feed_dict=feed_dict)\n",
    "        true_count += correctness\n",
    "    precision = float(true_count) / num_examples\n",
    "    tf.summary.scalar('precision', tf.constant(precision))\n",
    "    print('Num examples %d, Num Correct: %d Precision @ 1: %0.04f' %\n",
    "          (num_examples, true_count, precision))\n",
    "    return log\n",
    "\n",
    "def placeholder_inputs(batch_size):\n",
    "    images_placeholder = tf.placeholder(tf.float32, shape=(None, 28, 28, 1))\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(None))\n",
    "    return images_placeholder, labels_placeholder\n",
    "\n",
    "def fill_feed_dict(data_set, images_pl, labels_pl):\n",
    "    images_feed, labels_feed = data_set.next_batch(BATCH_SIZE)\n",
    "    feed_dict = {\n",
    "        images_pl: np.reshape(images_feed, (-1,28,28,1)),\n",
    "        labels_pl: labels_feed,\n",
    "    }\n",
    "    return feed_dict\n",
    "\n",
    "def run_training(data_sets):\n",
    "    with tf.Graph().as_default():\n",
    "        images_placeholder, labels_placeholder = placeholder_inputs(BATCH_SIZE)\n",
    "        logits = network(images_placeholder)\n",
    "        loss = loss_metrics(logits, labels_placeholder)\n",
    "        train_op = training(loss)\n",
    "        eval_correct = evaluation(logits, labels_placeholder)\n",
    "        summary = tf.summary.merge_all()\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "        summary_writer = tf.summary.FileWriter(\"/tmp/tensorflow/mnist/log\",\n",
    "                                               graph=tf.get_default_graph())\n",
    "        test_writer = tf.summary.FileWriter(\"/tmp/tensorflow/mnist/log/validation\",\n",
    "                                            graph=tf.get_default_graph())\n",
    "        sess.run(init)\n",
    "        for step in range(MAX_STEPS):\n",
    "            start_time = time.time()\n",
    "            feed_dict = fill_feed_dict(data_sets.train,\n",
    "                                       images_placeholder,\n",
    "                                       labels_placeholder)\n",
    "            _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)\n",
    "            duration = time.time() - start_time\n",
    "            if step % 100 == 0:\n",
    "                print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "                summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_str, step)\n",
    "                summary_writer.flush()\n",
    "            if (step + 1) % 1000 == 0 or (step + 1) == MAX_STEPS:\n",
    "                checkpoint_file = os.path.join(\"/tmp/tensorflow/mnist/log\", \"model.ckpt\")\n",
    "                saver.save(sess, checkpoint_file, global_step=step)\n",
    "                print('Validation Data Eval:')\n",
    "                log = do_eval(sess,\n",
    "                              eval_correct,\n",
    "                              images_placeholder,\n",
    "                              labels_placeholder,\n",
    "                              data_sets.validation,\n",
    "                              summary)\n",
    "                test_writer.add_summary(log, step)\n",
    "        # Return sess\n",
    "        graphdef = tf.get_default_graph().as_graph_def()\n",
    "        frozen_graph = tf.graph_util.convert_variables_to_constants(sess,\n",
    "                                                                    graphdef,\n",
    "                                                                    OUTPUT_NAMES)\n",
    "        return tf.graph_util.remove_training_nodes(frozen_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b38264a-4bd2-46e6-9088-af3e4d5d8c2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-48c9334849e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMNIST_DATASETS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mnist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMNIST_DATASETS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'contrib'"
     ]
    }
   ],
   "source": [
    "MNIST_DATASETS = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "tf_model = run_training(MNIST_DATASETS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
